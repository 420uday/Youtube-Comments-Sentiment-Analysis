{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install google-api-python-client nltk contractions emoji tensorflow transformers gradio seaborn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rpjj1S8Z1KS1",
        "outputId": "45449827-1993-40a2-b04e-61278a5e816b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (2.164.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.11/dist-packages (0.1.73)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.11/dist-packages (2.14.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.23.3)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (0.22.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (2.38.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (0.2.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (2.24.2)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (4.1.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.8.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.1)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.4)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.1)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.11/dist-packages (from seaborn) (3.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.69.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.26.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (4.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client) (3.2.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.6.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YouTube Comments Sentiment Analysis Tool\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential, load_model, Model\n",
        "from tensorflow.keras.layers import (Embedding, LSTM, Dense, Dropout,\n",
        "                                    Bidirectional, Input, concatenate)\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import contractions\n",
        "import emoji\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "import gradio as gr\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Custom sentence tokenizer fallback\n",
        "def custom_sent_tokenize(text, language='english'):\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    return [s for s in sentences if s.strip()]\n",
        "\n",
        "try:\n",
        "    nltk.download('punkt_tab')\n",
        "except:\n",
        "    import nltk.tokenize\n",
        "    nltk.tokenize.sent_tokenize = custom_sent_tokenize\n",
        "\n",
        "# Load external configurations\n",
        "def load_config(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            return json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Warning: {file_path} not found.\")\n",
        "        return {}\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error: {file_path} is not valid JSON.\")\n",
        "        return {}\n",
        "\n",
        "# Initialize components\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "indian_slangs = load_config('indian_slangs.json')\n",
        "\n",
        "# YouTube API setup\n",
        "API_KEY = 'AIzaSyCAygSBfRdqjdF2s11vQ_Or3h_cCJeEGrA'\n",
        "youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
        "\n",
        "def extract_video_id(url):\n",
        "    patterns = [\n",
        "        r'(?:v=|\\/)([0-9A-Za-z_-]{11}).*',\n",
        "        r'(?:embed\\/)([0-9A-Za-z_-]{11})',\n",
        "        r'(?:shorts\\/)([0-9A-Za-z_-]{11})',\n",
        "        r'youtu\\.be\\/([0-9A-Za-z_-]{11})'\n",
        "    ]\n",
        "    for pattern in patterns:\n",
        "        match = re.search(pattern, url)\n",
        "        if match:\n",
        "            return match.group(1)\n",
        "    return None\n",
        "\n",
        "def get_video_title(video_id):\n",
        "    try:\n",
        "        response = youtube.videos().list(part='snippet', id=video_id).execute()\n",
        "        return response['items'][0]['snippet']['title'] if 'items' in response else \"Title not found\"\n",
        "    except HttpError as e:\n",
        "        return f\"Error retrieving title: {str(e)}\"\n",
        "\n",
        "def get_video_comments(video_id, max_comments=100):\n",
        "    comments = []\n",
        "    next_page_token = None\n",
        "\n",
        "    while len(comments) < max_comments:\n",
        "        try:\n",
        "            response = youtube.commentThreads().list(\n",
        "                part='snippet',\n",
        "                videoId=video_id,\n",
        "                maxResults=min(100, max_comments - len(comments)),\n",
        "                pageToken=next_page_token,\n",
        "                textFormat='plainText'\n",
        "            ).execute()\n",
        "\n",
        "            for item in response['items']:\n",
        "                comment = item['snippet']['topLevelComment']['snippet']\n",
        "                comments.append({\n",
        "                    'author': comment['authorDisplayName'],\n",
        "                    'text': comment['textDisplay'],\n",
        "                    'likes': comment['likeCount'],\n",
        "                    'published_at': comment['publishedAt']\n",
        "                })\n",
        "\n",
        "            next_page_token = response.get('nextPageToken')\n",
        "            if not next_page_token or len(comments) >= max_comments:\n",
        "                break\n",
        "\n",
        "        except HttpError as e:\n",
        "            print(f\"Error fetching comments: {e}\")\n",
        "            break\n",
        "\n",
        "    return comments\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = contractions.fix(text)\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    text = emoji.demojize(text)\n",
        "\n",
        "    for slang, meaning in indian_slangs.items():\n",
        "        text = re.sub(r'\\b' + slang + r'\\b', meaning, text)\n",
        "\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and len(word) > 1]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "def load_training_videos(file_path='training_videos.json'):\n",
        "    config = load_config(file_path)\n",
        "    if not config:\n",
        "        return []\n",
        "\n",
        "    videos = []\n",
        "    for video in config.get('training_videos', []):\n",
        "        video_id = extract_video_id(video['url'])\n",
        "        if video_id:\n",
        "            videos.append({\n",
        "                'id': video_id,\n",
        "                'metadata': {\n",
        "                    'language': video.get('language', 'hindi'),\n",
        "                    'category': video.get('category', 'general')\n",
        "                }\n",
        "            })\n",
        "    return videos\n",
        "\n",
        "def generate_training_data():\n",
        "    training_videos = load_training_videos()\n",
        "\n",
        "    if not training_videos:\n",
        "        print(\"Warning: No training videos configured, using fallback defaults\")\n",
        "        training_videos = [{'id': vid} for vid in [\n",
        "            'KVh4KtUSW3A', 'JKa05nyUmuQ', 'H9154xIoYTA'\n",
        "        ]]\n",
        "\n",
        "    all_comments = []\n",
        "    for video in training_videos:\n",
        "        comments = get_video_comments(video['id'], max_comments=100)\n",
        "        for comment in comments:\n",
        "            if comment['text'].strip():\n",
        "                all_comments.append({\n",
        "                    'text': comment['text'],\n",
        "                    'metadata': video.get('metadata', {})\n",
        "                })\n",
        "\n",
        "    processed_comments = [preprocess_text(comment['text']) for comment in all_comments]\n",
        "\n",
        "    # Enhanced labeler with metadata awareness\n",
        "    def label_comment(text, metadata):\n",
        "        # Language-specific sentiment words\n",
        "        lang = metadata.get('language', 'hindi')\n",
        "        category = metadata.get('category', 'general')\n",
        "\n",
        "        if lang == 'hindi':\n",
        "            positive_words = ['mast', 'badhiya', 'shandaar', 'awesome', 'best', '‡§Ö‡§ö‡•ç‡§õ‡§æ', '‡§¨‡•á‡§π‡§§‡§∞‡•Ä‡§®']\n",
        "            negative_words = ['bakwas', 'ganda', 'worst', 'boring', '‡§ñ‡§∞‡§æ‡§¨', '‡§¨‡•á‡§ï‡§æ‡§∞']\n",
        "        else:\n",
        "            positive_words = ['great', 'awesome', 'love', 'best', 'excellent']\n",
        "            negative_words = ['bad', 'worst', 'hate', 'awful', 'poor']\n",
        "\n",
        "        # Category-specific adjustments\n",
        "        if category == 'comic':\n",
        "            positive_words.extend(['funny', '‡§π‡§Ç‡§∏‡•Ä', '‡§Æ‡§ú‡§æ‡§ï'])\n",
        "            negative_words.extend(['unfunny', '‡§¨‡•ã‡§∞‡§ø‡§Ç‡§ó'])\n",
        "        elif category == 'horror':\n",
        "            positive_words.extend(['scary', '‡§°‡§∞‡§æ‡§µ‡§®‡§æ', 'thrilling'])\n",
        "            negative_words.extend(['boring', 'unscary'])\n",
        "\n",
        "        text_lower = text.lower()\n",
        "        pos_score = sum(1 for word in positive_words if word in text_lower)\n",
        "        neg_score = sum(1 for word in negative_words if word in text_lower)\n",
        "\n",
        "        # Additional rules for strong sentiment\n",
        "        if 'masterpiece' in text_lower or '‡§¨‡•á‡§π‡§§‡§∞‡•Ä‡§®' in text_lower:\n",
        "            pos_score += 2\n",
        "        if 'trash' in text_lower or '‡§¨‡•á‡§ï‡§æ‡§∞' in text_lower:\n",
        "            neg_score += 2\n",
        "\n",
        "        if pos_score > neg_score:\n",
        "            return 2  # positive\n",
        "        elif neg_score > pos_score:\n",
        "            return 0  # negative\n",
        "        return 1  # neutral\n",
        "\n",
        "    labels = [\n",
        "        label_comment(text, comment['metadata'])\n",
        "        for text, comment in zip(processed_comments, all_comments)\n",
        "    ]\n",
        "\n",
        "    # Create metadata features\n",
        "    metadata_features = []\n",
        "    for comment in all_comments:\n",
        "        meta = comment['metadata']\n",
        "        features = [\n",
        "            1 if meta.get('language') == 'hindi' else 0,\n",
        "            1 if meta.get('category') in ['comic', 'music'] else 0,\n",
        "            1 if meta.get('category') in ['horror', 'geopolitics'] else 0\n",
        "        ]\n",
        "        metadata_features.append(features)\n",
        "\n",
        "    return processed_comments, labels, np.array(metadata_features)\n",
        "\n",
        "class EnhancedSentimentModel:\n",
        "    def __init__(self, model_path=None):\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.max_length = 100\n",
        "        self.model_path = model_path\n",
        "        self.labels = ['negative', 'neutral', 'positive']\n",
        "\n",
        "    def build_model(self, vocab_size=10000, embedding_dim=128):\n",
        "        # Text input branch\n",
        "        text_input = Input(shape=(self.max_length,), name='text_input')\n",
        "        x = Embedding(vocab_size, embedding_dim)(text_input)\n",
        "        x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
        "        x = Bidirectional(LSTM(32))(x)\n",
        "\n",
        "        # Metadata input branch\n",
        "        meta_input = Input(shape=(3,), name='metadata_input')\n",
        "\n",
        "        # Merge branches\n",
        "        merged = concatenate([x, meta_input])\n",
        "\n",
        "        # Dense layers\n",
        "        x = Dense(64, activation='relu')(merged)\n",
        "        x = Dropout(0.5)(x)\n",
        "        output = Dense(3, activation='softmax')(x)\n",
        "\n",
        "        self.model = Model(inputs=[text_input, meta_input], outputs=output)\n",
        "        self.model.compile(\n",
        "            loss='categorical_crossentropy',\n",
        "            optimizer='adam',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "        return self.model\n",
        "\n",
        "    def train(self, texts, labels, metadata, epochs=10, batch_size=32):\n",
        "        self.tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
        "        self.tokenizer.fit_on_texts(texts)\n",
        "\n",
        "        sequences = self.tokenizer.texts_to_sequences(texts)\n",
        "        padded_sequences = pad_sequences(sequences, maxlen=self.max_length, padding='post', truncating='post')\n",
        "        categorical_labels = tf.keras.utils.to_categorical(labels, num_classes=3)\n",
        "\n",
        "        X_train, X_val, m_train, m_val, y_train, y_val = train_test_split(\n",
        "            padded_sequences, metadata, categorical_labels,\n",
        "            test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "        callbacks = [\n",
        "            EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
        "            ModelCheckpoint('enhanced_sentiment_model.h5', monitor='val_accuracy',\n",
        "                          save_best_only=True, mode='max')\n",
        "        ]\n",
        "\n",
        "        history = self.model.fit(\n",
        "            [X_train, m_train], y_train,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            validation_data=([X_val, m_val], y_val),\n",
        "            callbacks=callbacks\n",
        "        )\n",
        "\n",
        "        with open('enhanced_tokenizer.pickle', 'wb') as handle:\n",
        "            pickle.dump(self.tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "        return history\n",
        "\n",
        "    def load(self):\n",
        "        if self.model_path and os.path.exists(self.model_path):\n",
        "            self.model = load_model(self.model_path)\n",
        "            try:\n",
        "                with open('enhanced_tokenizer.pickle', 'rb') as handle:\n",
        "                    self.tokenizer = pickle.load(handle)\n",
        "            except:\n",
        "                print(\"Warning: Could not load tokenizer\")\n",
        "        else:\n",
        "            model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            self.model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "    def predict(self, texts, metadata=None):\n",
        "        if isinstance(texts, str):\n",
        "            texts = [texts]\n",
        "\n",
        "        if isinstance(self.model, Model):  # Our custom model\n",
        "            processed_texts = [preprocess_text(text) for text in texts]\n",
        "            sequences = self.tokenizer.texts_to_sequences(processed_texts)\n",
        "            padded_sequences = pad_sequences(sequences, maxlen=self.max_length, padding='post', truncating='post')\n",
        "\n",
        "            # Default metadata if not provided\n",
        "            if metadata is None:\n",
        "                metadata = np.zeros((len(texts), 3))  # Neutral defaults\n",
        "\n",
        "            predictions = self.model.predict([padded_sequences, metadata])\n",
        "            predicted_classes = np.argmax(predictions, axis=1)\n",
        "            results = [self.labels[pred] for pred in predicted_classes]\n",
        "            confidence = np.max(predictions, axis=1)\n",
        "\n",
        "            # Post-processing for strong sentiment indicators\n",
        "            final_results = []\n",
        "            for text, (pred, conf) in zip(texts, zip(results, confidence)):\n",
        "                text_lower = text.lower()\n",
        "                if 'masterpiece' in text_lower and pred == 'neutral':\n",
        "                    final_results.append(('positive', min(1.0, conf + 0.3)))\n",
        "                elif 'trash' in text_lower and pred == 'neutral':\n",
        "                    final_results.append(('negative', min(1.0, conf + 0.3)))\n",
        "                else:\n",
        "                    final_results.append((pred, conf))\n",
        "            return final_results\n",
        "        else:  # Transformer model\n",
        "            results = []\n",
        "            for text in texts:\n",
        "                inputs = self.tokenizer(preprocess_text(text), return_tensors=\"tf\", padding=True, truncation=True)\n",
        "                outputs = self.model(inputs)\n",
        "                predictions = tf.nn.softmax(outputs.logits, axis=-1)\n",
        "                predicted_class = tf.argmax(predictions, axis=1).numpy()[0]\n",
        "                confidence = tf.reduce_max(predictions, axis=1).numpy()[0]\n",
        "                sentiment_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
        "                results.append((sentiment_map[predicted_class], confidence))\n",
        "            return results\n",
        "\n",
        "# Analyzing comments and visualizing results\n",
        "def analyze_comments(comments):\n",
        "    analyzer = EnhancedSentimentModel('enhanced_sentiment_model.h5')\n",
        "    try:\n",
        "        analyzer.load()\n",
        "        print(\"Loaded enhanced model successfully.\")\n",
        "    except:\n",
        "        print(\"Training a new enhanced model...\")\n",
        "        texts, labels, metadata = generate_training_data()\n",
        "        analyzer.build_model()\n",
        "        analyzer.train(texts, labels, metadata, epochs=5)\n",
        "\n",
        "    comment_texts = [comment['text'] for comment in comments]\n",
        "\n",
        "    # Create metadata for prediction (default to Hindi general)\n",
        "    pred_metadata = np.array([[1, 0, 0]] * len(comment_texts))  # All Hindi\n",
        "\n",
        "    sentiment_results = analyzer.predict(comment_texts, pred_metadata)\n",
        "\n",
        "    results = []\n",
        "    for i, (comment, (sentiment, confidence)) in enumerate(zip(comments, sentiment_results)):\n",
        "        results.append({\n",
        "            'author': comment['author'],\n",
        "            'text': comment['text'],\n",
        "            'likes': comment['likes'],\n",
        "            'sentiment': sentiment,\n",
        "            'confidence': float(confidence)\n",
        "        })\n",
        "\n",
        "    sentiment_counts = {\n",
        "        'positive': sum(1 for r in results if r['sentiment'] == 'positive'),\n",
        "        'neutral': sum(1 for r in results if r['sentiment'] == 'neutral'),\n",
        "        'negative': sum(1 for r in results if r['sentiment'] == 'negative')\n",
        "    }\n",
        "\n",
        "    total = len(results)\n",
        "    sentiment_percentages = {\n",
        "        'positive': (sentiment_counts['positive'] / total) * 100 if total > 0 else 0,\n",
        "        'neutral': (sentiment_counts['neutral'] / total) * 100 if total > 0 else 0,\n",
        "        'negative': (sentiment_counts['negative'] / total) * 100 if total > 0 else 0\n",
        "    }\n",
        "\n",
        "    top_comments = sorted(results, key=lambda x: x['likes'], reverse=True)[:10]\n",
        "\n",
        "    return {\n",
        "        'results': results,\n",
        "        'top_comments': top_comments,\n",
        "        'sentiment_counts': sentiment_counts,\n",
        "        'sentiment_percentages': sentiment_percentages\n",
        "    }\n",
        "\n",
        "# Function to generate summary\n",
        "def generate_summary(analysis_results, video_title):\n",
        "    \"\"\"Generate a summary of sentiment analysis results.\"\"\"\n",
        "    results = analysis_results['results']\n",
        "    sentiment_percentages = analysis_results['sentiment_percentages']\n",
        "\n",
        "    # Overall sentiment\n",
        "    dominant_sentiment = max(sentiment_percentages.items(), key=lambda x: x[1])[0]\n",
        "\n",
        "    # Calculate average confidence\n",
        "    avg_confidence = sum(r['confidence'] for r in results) / len(results) if results else 0\n",
        "\n",
        "    # Example summary template\n",
        "    summary = f\"\"\"\n",
        "## Sentiment Analysis Summary for: \"{video_title}\"\n",
        "\n",
        "### Overall Sentiment: {dominant_sentiment.capitalize()}\n",
        "\n",
        "- **Positive comments**: {sentiment_percentages['positive']:.1f}%\n",
        "- **Neutral comments**: {sentiment_percentages['neutral']:.1f}%\n",
        "- **Negative comments**: {sentiment_percentages['negative']:.1f}%\n",
        "\n",
        "The average confidence of sentiment predictions is {avg_confidence:.2f}.\n",
        "\n",
        "### Key Observations:\n",
        "- The dominant sentiment is {dominant_sentiment}, suggesting viewers generally {'enjoyed' if dominant_sentiment == 'positive' else 'had mixed feelings about' if dominant_sentiment == 'neutral' else 'disliked'} this content.\n",
        "- {'Most comments were positive, indicating good reception.' if sentiment_percentages['positive'] > 50 else ''}\n",
        "- {'There is a significant amount of neutral comments, suggesting room for improvement.' if sentiment_percentages['neutral'] > 30 else ''}\n",
        "- {'The negative comments suggest areas of concern that may need addressing.' if sentiment_percentages['negative'] > 20 else ''}\n",
        "\n",
        "### Recommendation:\n",
        "{'Consider highlighting the positive aspects that viewers appreciated.' if dominant_sentiment == 'positive' else 'Consider clarifying content that might be causing neutral reactions.' if dominant_sentiment == 'neutral' else 'Consider addressing criticism in future content.'}\n",
        "\"\"\"\n",
        "    return summary\n",
        "\n",
        "# Create visualization functions\n",
        "def create_sentiment_chart(sentiment_percentages):\n",
        "    \"\"\"Create a pie chart of sentiment percentages.\"\"\"\n",
        "    labels = list(sentiment_percentages.keys())\n",
        "    sizes = list(sentiment_percentages.values())\n",
        "    colors = ['#ff9999', '#66b3ff', '#99ff99']\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    ax.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
        "    ax.axis('equal')\n",
        "    plt.title('Sentiment Distribution')\n",
        "\n",
        "    return fig\n",
        "\n",
        "# Gradio Interface Functions\n",
        "def analyze_youtube_url(youtube_url):\n",
        "    \"\"\"Main function to analyze YouTube comments from a URL.\"\"\"\n",
        "    video_id = extract_video_id(youtube_url)\n",
        "    if not video_id:\n",
        "        return \"Invalid YouTube URL. Please provide a valid YouTube video URL.\", None, None, None\n",
        "\n",
        "    video_title = get_video_title(video_id)\n",
        "    comments = get_video_comments(video_id, max_comments=100)\n",
        "\n",
        "    if not comments:\n",
        "        return \"No comments found or error retrieving comments.\", None, None, None\n",
        "\n",
        "    analysis_results = analyze_comments(comments)\n",
        "    summary = generate_summary(analysis_results, video_title)\n",
        "    chart = create_sentiment_chart(analysis_results['sentiment_percentages'])\n",
        "\n",
        "    # Format top comments for display\n",
        "    top_comments_md = \"## Top Comments\\n\\n\"\n",
        "    for i, comment in enumerate(analysis_results['top_comments'][:5], 1):\n",
        "        sentiment_emoji = \"üòä\" if comment['sentiment'] == 'positive' else \"üòê\" if comment['sentiment'] == 'neutral' else \"üòû\"\n",
        "        top_comments_md += f\"### {i}. {comment['author']} ({sentiment_emoji} {comment['sentiment'].capitalize()})\\n\"\n",
        "        top_comments_md += f\"{comment['text']}\\n\"\n",
        "        top_comments_md += f\"*Likes: {comment['likes']}*\\n\\n\"\n",
        "\n",
        "    return summary, top_comments_md, chart, f\"Analysis complete for video: {video_title}\"\n",
        "\n",
        "# Create Gradio Interface\n",
        "def create_gradio_interface():\n",
        "    \"\"\"Create and launch the Gradio interface.\"\"\"\n",
        "    # CSS for customizing the interface\n",
        "    css = \"\"\"\n",
        "    .gradio-container {\n",
        "        font-family: 'Arial', sans-serif;\n",
        "    }\n",
        "    h1 {\n",
        "        color: #4285F4;\n",
        "        text-align: center;\n",
        "    }\n",
        "    \"\"\"\n",
        "\n",
        "    # Create the interface\n",
        "    with gr.Blocks(css=css) as demo:\n",
        "        gr.Markdown(\"# YouTube Comments Sentiment Analyzer\")\n",
        "        gr.Markdown(\"Enter a YouTube video URL to analyze the sentiment of its comments\")\n",
        "\n",
        "        with gr.Row():\n",
        "            youtube_url = gr.Textbox(label=\"YouTube Video URL\", placeholder=\"https://www.youtube.com/watch?v=...\")\n",
        "            analyze_button = gr.Button(\"Analyze\")\n",
        "\n",
        "        status = gr.Textbox(label=\"Status\")\n",
        "\n",
        "        with gr.Tabs():\n",
        "            with gr.TabItem(\"Summary\"):\n",
        "                summary_output = gr.Markdown()\n",
        "\n",
        "            with gr.TabItem(\"Top Comments\"):\n",
        "                top_comments = gr.Markdown()\n",
        "\n",
        "            with gr.TabItem(\"Visualization\"):\n",
        "                chart_output = gr.Plot()\n",
        "\n",
        "        analyze_button.click(\n",
        "            fn=analyze_youtube_url,\n",
        "            inputs=youtube_url,\n",
        "            outputs=[summary_output, top_comments, chart_output, status]\n",
        "        )\n",
        "\n",
        "        gr.Markdown(\"## How to use\")\n",
        "        gr.Markdown(\"\"\"\n",
        "        1. Paste a YouTube video URL in the input box\n",
        "        2. Click the 'Analyze' button\n",
        "        3. Wait for the analysis to complete\n",
        "        4. View the results in the tabs below\n",
        "        \"\"\")\n",
        "\n",
        "    return demo\n",
        "\n",
        "# Main function to run the application\n",
        "def main():\n",
        "    \"\"\"Main function to run the YouTube Comments Sentiment Analyzer.\"\"\"\n",
        "    # Create and launch the Gradio interface\n",
        "    demo = create_gradio_interface()\n",
        "    demo.launch(debug=True, share=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 906
        },
        "id": "xtB15MQd1SG9",
        "outputId": "5042ae93-6004-4bd3-ffbf-4fac879bc8d2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: indian_slangs.json not found.\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://49a9c6c4d96d0bc36d.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://49a9c6c4d96d0bc36d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded enhanced model successfully.\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://49a9c6c4d96d0bc36d.gradio.live\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
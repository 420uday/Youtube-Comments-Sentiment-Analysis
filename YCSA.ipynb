{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "v_khta1KJW9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy matplotlib nltk scikit-learn kaggle youtube-transcript-api gradio imbalanced-learn\n",
        "!pip install tensorflow.keras\n",
        "\n",
        "# Verify installations\n",
        "import tensorflow as tf\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
      ],
      "metadata": {
        "id": "IQvP4dLUK9Rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Consolidate into one configuration block\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        # Enable memory growth first\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "        # Then set memory limit if needed\n",
        "        tf.config.experimental.set_virtual_device_configuration(\n",
        "            gpus[0],\n",
        "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=7*1024)]\n",
        "        )\n",
        "        print(\"GPU configured with memory growth and 7GB limit\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"Error configuring GPU: {e}\")"
      ],
      "metadata": {
        "id": "8rBxE1-AKsCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting Dataset"
      ],
      "metadata": {
        "id": "HlRAKvmBLINY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Download Dataset\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Download dataset\n",
        "!kaggle datasets download atifaliak/youtube-comments-dataset\n",
        "!unzip youtube-comments-dataset.zip"
      ],
      "metadata": {
        "id": "1dGKLpPmLDGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "DUxkx8XzLk3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset - adjust path if needed\n",
        "try:\n",
        "    df = pd.read_csv('YoutubeCommentsDataSet.csv')\n",
        "except FileNotFoundError:\n",
        "    try:\n",
        "        df = pd.read_csv('./YoutubeCommentsDataSet.csv')\n",
        "    except FileNotFoundError:\n",
        "        print(\"Dataset file not found. Please upload or download it first.\")\n",
        "        # Create a sample dataframe for testing\n",
        "        df = pd.DataFrame({\n",
        "            'Comment': ['This is great!', 'I hate this', 'Not sure what to think'],\n",
        "            'Sentiment': [2, 0, 1]  # 0=negative, 1=neutral, 2=positive\n",
        "        })\n",
        "\n",
        "# Check structure\n",
        "print(df.head())\n",
        "print(f\"\\nClass distribution:\\n{df['Sentiment'].value_counts()}\")\n",
        "\n",
        "# Calculate class weights\n",
        "from sklearn.utils import class_weight\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(df['Sentiment']),\n",
        "    y=df['Sentiment']\n",
        ")\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "print(f\"\\nClass weights: {class_weights}\")"
      ],
      "metadata": {
        "id": "BmQXr0HmLjbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Processing for LSTM"
      ],
      "metadata": {
        "id": "6nukpZmILp9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Initialize tokenizer with error handling\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(df['Comment'].astype(str))\n",
        "\n",
        "# Convert text to sequences\n",
        "sequences = tokenizer.texts_to_sequences(df['Comment'].astype(str))\n",
        "padded_sequences = pad_sequences(sequences, maxlen=200, truncating='post', padding='post')\n",
        "\n",
        "# Vocabulary info\n",
        "word_index = tokenizer.word_index\n",
        "print(f\"Vocabulary size: {len(word_index)}\")\n",
        "\n",
        "# Train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Import properly from imblearn with error handling\n",
        "try:\n",
        "    from imblearn.over_sampling import RandomOverSampler\n",
        "except ImportError:\n",
        "    print(\"Installing imblearn...\")\n",
        "    from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "X = padded_sequences\n",
        "y = pd.get_dummies(df['Sentiment']).values  # One-hot encoding\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Oversample minority classes\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)"
      ],
      "metadata": {
        "id": "VQK5EfSuLuQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BiLSTM Architecture"
      ],
      "metadata": {
        "id": "srbD-DjaMCPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "def build_optimized_model(vocab_size):\n",
        "    model = Sequential([\n",
        "        # Input layer\n",
        "        Embedding(input_dim=vocab_size + 1,\n",
        "                  output_dim=128,\n",
        "                  mask_zero=True,\n",
        "                  input_length=200),\n",
        "\n",
        "        # First layer with normalization and dropout\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        # First bidirectional LSTM layer\n",
        "        Bidirectional(LSTM(128, return_sequences=True,\n",
        "                       kernel_regularizer=l2(0.01),\n",
        "                       recurrent_dropout=0.2)),\n",
        "\n",
        "        # More regularization\n",
        "        Dropout(0.4),\n",
        "        BatchNormalization(),\n",
        "\n",
        "        # Second bidirectional LSTM layer\n",
        "        Bidirectional(LSTM(64,\n",
        "                       kernel_regularizer=l2(0.01),\n",
        "                       recurrent_dropout=0.2)),\n",
        "\n",
        "        # Final dense layers\n",
        "        Dropout(0.4),\n",
        "        Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "        BatchNormalization(),\n",
        "        Dense(3, activation='softmax', dtype='float32') # Force float32 output\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the model\n",
        "model = build_optimized_model(len(word_index))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "kN4iYCK6MF_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.metrics import Metric\n",
        "\n",
        "class F1Score(Metric):\n",
        "    def __init__(self, num_classes, average='macro', name='f1_score', **kwargs):\n",
        "        super(F1Score, self).__init__(name=name, **kwargs)\n",
        "        self.num_classes = num_classes\n",
        "        self.average = average\n",
        "        self.precision = tf.keras.metrics.Precision()\n",
        "        self.recall = tf.keras.metrics.Recall()\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        y_pred = tf.argmax(y_pred, axis=-1)\n",
        "        y_true = tf.argmax(y_true, axis=-1)\n",
        "\n",
        "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
        "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
        "\n",
        "    def result(self):\n",
        "        p = self.precision.result()\n",
        "        r = self.recall.result()\n",
        "        return 2 * ((p * r) / (p + r + tf.keras.backend.epsilon()))\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.precision.reset_state()\n",
        "        self.recall.reset_state()"
      ],
      "metadata": {
        "id": "SE5LUbQSMLJ9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import mixed_precision\n",
        "policy = mixed_precision.Policy('mixed_float16')\n",
        "mixed_precision.set_global_policy(policy)\n",
        "\n",
        "# Set training parameters\n",
        "initial_learning_rate = 0.0005\n",
        "epochs = 50  # Reduced from 100 to avoid long training times\n",
        "batch_size = 128  # Changed for better performance\n",
        "\n",
        "# Optimizer with gradient clipping to prevent exploding gradients\n",
        "optimizer = Adam(learning_rate=initial_learning_rate, clipnorm=1.0)\n",
        "\n",
        "# Compile model with appropriate metrics\n",
        "# Compile model with appropriate metrics\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=optimizer,\n",
        "    metrics=['accuracy',\n",
        "             tf.keras.metrics.Precision(name='precision'),\n",
        "             tf.keras.metrics.Recall(name='recall'),\n",
        "             F1Score(num_classes=3, average='macro', name='f1_score')],\n",
        "    jit_compile=True  # Enable XLA compilation for speed\n",
        ")\n",
        "\n",
        "# Adjusted class weights based on dataset imbalance\n",
        "class_weights = {\n",
        "    0: 2.5,  # Negative\n",
        "    1: 1.0,  # Neutral\n",
        "    2: 0.8   # Positive\n",
        "}"
      ],
      "metadata": {
        "id": "oWVQ-JcvQg9i"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create callbacks for better training\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "callbacks = [\n",
        "    # Stop training when validation loss stops improving\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=5,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "\n",
        "    # Save the best model\n",
        "    ModelCheckpoint(\n",
        "        'best_model.h5',\n",
        "        monitor='val_loss',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "\n",
        "    # Reduce learning rate when plateauing\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=3,\n",
        "        min_lr=1e-6,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "# Train the model with class weights and callbacks\n",
        "\n",
        "# Use X_train_resampled and y_train_resampled for balanced training\n",
        "try:\n",
        "    history = model.fit(\n",
        "        X_train_resampled, y_train_resampled,\n",
        "        validation_data=(X_test, y_test),\n",
        "        epochs=20,\n",
        "        batch_size=64,\n",
        "        class_weight=class_weights,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"Error during training: {e}\")\n",
        "    print(\"\\nTrying without resampled data...\")\n",
        "\n",
        "    # Fallback to original data if resampling causes issues\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_test, y_test),\n",
        "        epochs=20,\n",
        "        batch_size=64,\n",
        "        class_weight=class_weights,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )"
      ],
      "metadata": {
        "id": "H0qjcSssMPHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save and Load Model"
      ],
      "metadata": {
        "id": "jEzPbBm-MqGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "import pickle\n",
        "\n",
        "# Save model and tokenizer\n",
        "model.save('youtube_sentiment_bilstm.h5')\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = load_model('youtube_sentiment_bilstm.h5')\n",
        "with open('tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)"
      ],
      "metadata": {
        "id": "3i37ln6UMkPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "Ggjgx0ETMRDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Classification report\n",
        "print(classification_report(y_true_classes, y_pred_classes,\n",
        "                          target_names=['negative', 'neutral', 'positive']))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['negative', 'neutral', 'positive'],\n",
        "            yticklabels=['negative', 'neutral', 'positive'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7vSDIxlOMT1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Learning Curve Visualization"
      ],
      "metadata": {
        "id": "txzYbEpGMWHd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Accuracy plot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "\n",
        "# Loss plot\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_n0qUqrjMT8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Youtube API Integration"
      ],
      "metadata": {
        "id": "VrKd6j2HMZ-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "from google.colab import userdata\n",
        "\n",
        "API_KEY = userdata.get('youtube_api_key')\n",
        "\n",
        "def get_video_comments(video_url, max_results=100, api_key=None):\n",
        "    \"\"\"Get comments from a YouTube video using the YouTube API\"\"\"\n",
        "    if api_key is None:\n",
        "        print(\"Warning: No API key provided\")\n",
        "        return []\n",
        "\n",
        "    try:\n",
        "        from googleapiclient.discovery import build\n",
        "        video_id = video_url.split('v=')[1].split('&')[0] if 'v=' in video_url else video_url.split('/')[-1]\n",
        "        youtube = build('youtube', 'v3', developerKey=api_key)\n",
        "\n",
        "        comments = []\n",
        "        next_page_token = None\n",
        "\n",
        "        while len(comments) < max_results:\n",
        "            request = youtube.commentThreads().list(\n",
        "                part='snippet',\n",
        "                videoId=video_id,\n",
        "                maxResults=min(100, max_results - len(comments)),\n",
        "                pageToken=next_page_token,\n",
        "                textFormat='plainText'\n",
        "            )\n",
        "            response = request.execute()\n",
        "\n",
        "            for item in response['items']:\n",
        "                comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
        "                comments.append(comment)\n",
        "\n",
        "            next_page_token = response.get('nextPageToken')\n",
        "            if not next_page_token:\n",
        "                break\n",
        "\n",
        "        return comments[:max_results]\n",
        "    except Exception as e:\n",
        "        print(f\"Error retrieving comments: {e}\")\n",
        "        return []\n",
        "\n",
        "# Alternative function using youtube-transcript-api for videos without API key\n",
        "def get_video_comments_alt(video_url, max_results=100):\n",
        "    \"\"\"Alternative comment retrieval using youtube-transcript-api\"\"\"\n",
        "    try:\n",
        "        from youtube_transcript_api import YouTubeTranscriptApi\n",
        "\n",
        "        # Extract video ID from URL\n",
        "        if 'v=' in video_url:\n",
        "            video_id = video_url.split('v=')[1].split('&')[0]\n",
        "        elif 'youtu.be/' in video_url:\n",
        "            video_id = video_url.split('youtu.be/')[1].split('?')[0]\n",
        "        else:\n",
        "            video_id = video_url\n",
        "\n",
        "        # Get transcript as comments\n",
        "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "        comments = [item['text'] for item in transcript[:max_results]]\n",
        "        return comments\n",
        "    except Exception as e:\n",
        "        print(f\"Error retrieving transcript: {e}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "GtCqGxkfMc5t"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization Function"
      ],
      "metadata": {
        "id": "X9kZ7vROMuEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualization Functions\n",
        "import plotly.express as px\n",
        "\n",
        "def create_sentiment_chart(results):\n",
        "    \"\"\"Create interactive visualizations\"\"\"\n",
        "    stats = results['stats']\n",
        "\n",
        "    # Pie chart\n",
        "    fig_pie = px.pie(\n",
        "        values=[stats['positive'], stats['negative'], stats['neutral']],\n",
        "        names=['Positive', 'Negative', 'Neutral'],\n",
        "        title='Sentiment Distribution',\n",
        "        color=['Positive', 'Negative', 'Neutral'],\n",
        "        color_discrete_map={'Positive':'green', 'Negative':'red', 'Neutral':'gray'}\n",
        "    )\n",
        "\n",
        "    # Confidence distribution\n",
        "    confidences = [max(p) for p in results['probabilities']]\n",
        "    fig_hist = px.histogram(\n",
        "        x=confidences,\n",
        "        nbins=20,\n",
        "        title='Prediction Confidence Distribution',\n",
        "        labels={'x': 'Confidence Score'}\n",
        "    )\n",
        "\n",
        "    return fig_pie, fig_hist\n",
        "\n",
        "def show_sample_comments(results, sentiment_type='positive', n=3):\n",
        "    \"\"\"Display sample comments for each sentiment\"\"\"\n",
        "    comments = results['comments']\n",
        "    sentiments = results['sentiments']\n",
        "\n",
        "    sample = [\n",
        "        (comment, confidence)\n",
        "        for comment, sent, confidence in zip(comments, sentiments, results['probabilities'])\n",
        "        if sent == sentiment_type\n",
        "    ][:n]\n",
        "\n",
        "    print(f\"\\nSample {sentiment_type} comments:\")\n",
        "    for i, (comment, prob) in enumerate(sample, 1):\n",
        "        print(f\"\\n{i}. [{np.max(prob):.1%} confidence]\")\n",
        "        print(comment)\n"
      ],
      "metadata": {
        "id": "h6zgyaVpMwV7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradion Interface"
      ],
      "metadata": {
        "id": "T2wlFCSjMxss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import re\n",
        "\n",
        "def validate_youtube_url(url):\n",
        "    patterns = [\n",
        "        r'(https?://)?(www\\.)?youtube\\.com/watch\\?v=([^&]+)',\n",
        "        r'(https?://)?(www\\.)?youtu\\.be/([^?]+)'\n",
        "    ]\n",
        "    return any(re.match(pattern, url) for pattern in patterns)\n",
        "\n",
        "def analyze_youtube_video(video_url, api_key=None, max_comments=100):\n",
        "    \"\"\"Main analysis function\"\"\"\n",
        "    # Get comments\n",
        "    if api_key:\n",
        "        comments = get_video_comments(video_url, max_results=max_comments, api_key=api_key)\n",
        "    else:\n",
        "        comments = get_video_comments_alt(video_url, max_results=max_comments)\n",
        "\n",
        "    if not comments:\n",
        "        return None, \"No comments found or error retrieving comments\"\n",
        "\n",
        "    # Preprocess and predict\n",
        "    sequences = tokenizer.texts_to_sequences(comments)\n",
        "    padded = pad_sequences(sequences, maxlen=200, truncating='post', padding='post')\n",
        "    predictions = model.predict(padded)\n",
        "\n",
        "    # Process results\n",
        "    sentiments = np.argmax(predictions, axis=1)\n",
        "    sentiment_labels = ['negative', 'neutral', 'positive']\n",
        "    labeled_sentiments = [sentiment_labels[s] for s in sentiments]\n",
        "\n",
        "    # Calculate stats\n",
        "    total = len(sentiments)\n",
        "    stats = {\n",
        "        'total_comments': total,\n",
        "        'positive': 100 * np.sum(sentiments == 2) / total,\n",
        "        'negative': 100 * np.sum(sentiments == 0) / total,\n",
        "        'neutral': 100 * np.sum(sentiments == 1) / total\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        'comments': comments,\n",
        "        'sentiments': labeled_sentiments,\n",
        "        'probabilities': predictions,\n",
        "        'stats': stats\n",
        "    }, None\n",
        "\n",
        "def analyze_with_gradio(video_url, api_key=None):\n",
        "    if not validate_youtube_url(video_url):\n",
        "        return \"Invalid YouTube URL format\", None, None\n",
        "    results, error = analyze_youtube_video(video_url, api_key=api_key)\n",
        "    if error:\n",
        "        return error, None, None\n",
        "\n",
        "    pie, hist = create_sentiment_chart(results)\n",
        "\n",
        "    output = f\"\"\"\n",
        "    ## Analysis Results\n",
        "    - Total Comments Analyzed: {results['stats']['total_comments']}\n",
        "    - Positive: {results['stats']['positive']:.1f}%\n",
        "    - Negative: {results['stats']['negative']:.1f}%\n",
        "    - Neutral: {results['stats']['neutral']:.1f}%\n",
        "    \"\"\"\n",
        "\n",
        "    return output, pie, hist\n",
        "\n",
        "# Create and launch the interface\n",
        "iface = gr.Interface(\n",
        "    fn=analyze_with_gradio,\n",
        "    inputs=gr.Textbox(label=\"YouTube Video URL\"),\n",
        "    outputs=[\n",
        "        gr.Markdown(label=\"Summary\"),\n",
        "        gr.Plot(label=\"Sentiment Distribution\"),\n",
        "        gr.Plot(label=\"Confidence Scores\")\n",
        "    ],\n",
        "    title=\"YouTube Comment Sentiment Analysis\",\n",
        "    description=\"Analyze the sentiment of comments on any YouTube video\"\n",
        ")\n",
        "\n",
        "iface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "JhgYB4fvMziL",
        "outputId": "0ecd9d39-64e2-49a7-f9a3-7d54c8344809"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://62971abad0a0ed1be8.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://62971abad0a0ed1be8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}